model: # Model architecture
  type: basic # basic,improved, logistic, rf, xgb
  input_dim: 14
  hidden_dims: [64]
  dropout_rate: 0.7
  activation: relu # relu, gelu, leaky_relu, swish
  use_batch_norm: true
  use_residual: false

  # Shared defaults for any tree-based model
  tree_params:
    random_state: 42
    n_jobs: -1

  # Random Forest–specific
  rf_params:
  n_estimators: 400
  max_depth: 10
  min_samples_split: 2
  min_samples_leaf: 1
  class_weight: balanced # e.g. "balanced", null
  criterion: gini

  # XGBoost–specific
  xgb_params:
    n_estimators: 600
    learning_rate: 0.05
    max_depth: 6
    subsample: 0.8
    colsample_bytree: 0.8
    reg_lambda: 1.0
    scale_pos_weight: null # can auto-fill at runtime
    tree_method: "hist" # good default
    eval_metric: "logloss"

training: # Training parameters
  run_name: tokyo
  epochs: 25
  lr: 0.02
  loss_type: bce # 'bce', 'weighted_bce', 'focal'
  alpha: 0.25 # For focal/weighted
  gamma: 2.0
  optimizer_name: Adam # Adam, AdamW
  weight_decay: 0.001
  patience: 10
  min_delta: 0.0001
  use_class_weights: false
  use_scheduler: false
  scheduler_type: cosine # If true
  batch_size: 32

tuning: # grid search
  enabled: false # Set to true to run tuning
  lr_range: [0.0005, 0.001, 0.005, 0.01]
  hidden_dims_options: [[128], [256, 128], [512, 256, 128]]
  dropout_range: [0.2, 0.3, 0.4, 0.5]

preprocessing:
  save_dir: experiments/preprocessing/artifacts

inference:
  cost_false_positives: 10
  cost_false_negatives: 25
  benefit_true_positives: 20
  decision_threshold: 0.38999999999999985

data:
  filepath:
    state: experiments/preprocessing/artifacts/preprocessor_state.json
    train: experiments/preprocessing/artifacts/titanic_train_processed.xlsx
    val: experiments/preprocessing/artifacts/titanic_val_processed.xlsx
    test: experiments/preprocessing/artifacts/titanic_test_processed.xlsx
  target_column: Survived
