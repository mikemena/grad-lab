model: # Model architecture
  type: basic # basic,improved, logistic, rf, xgb
  input_dim: 14
  hidden_dims: [64]
  dropout_rate: 0.7
  activation: relu # relu, gelu, leaky_relu, swish
  use_batch_norm: true
  use_residual: false

training: # Training parameters
  run_name: tokyo
  epochs: 25
  lr: 0.02
  loss_type: bce # 'bce', 'weighted_bce', 'focal'
  alpha: 0.25 # For focal/weighted
  gamma: 2.0
  optimizer_name: Adam # Adam, AdamW
  weight_decay: 0.001
  patience: 10
  min_delta: 0.0001
  use_class_weights: false
  use_scheduler: false
  scheduler_type: cosine # If true
  batch_size: 32

tuning: # grid search
  enabled: false # Set to true to run tuning
  lr_range: [0.0005, 0.001, 0.005, 0.01]
  hidden_dims_options: [[128], [256, 128], [512, 256, 128]]
  dropout_range: [0.2, 0.3, 0.4, 0.5]

tree_params: # NEW: common patternâ€”optional section
  random_state: 42 # shared default

rf_params: # NEW: passed to RandomForestClassifier(**rf_params)
  n_estimators: 400
  max_depth: null
  min_samples_split: 2
  min_samples_leaf: 1
  class_weight: null # e.g. "balanced"
  n_jobs: -1

xgb_params: # NEW: passed to XGBClassifier(**xgb_params)
  n_estimators: 600
  learning_rate: 0.05
  max_depth: 6
  subsample: 0.9
  colsample_bytree: 0.9
  reg_lambda: 1.0
  tree_method: "hist" # good default
  n_jobs: -1
  eval_metric: "logloss"

preprocessing:
  save_dir: experiments/preprocessing/artifacts

inference:
  cost_false_positives: 10
  cost_false_negatives: 25
  benefit_true_positives: 20
  decision_threshold: 0.38999999999999985

data:
  filepath:
    state: experiments/preprocessing/artifacts/preprocessor_state.json
    train: experiments/preprocessing/artifacts/titanic_train_processed.xlsx
    val: experiments/preprocessing/artifacts/titanic_val_processed.xlsx
    test: experiments/preprocessing/artifacts/titanic_test_processed.xlsx
  target_column: Survived
